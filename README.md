# Pipeline de Dados do Mercado Livre

Este projeto implementa um pipeline completo de dados desde a extraÃ§Ã£o atÃ© a visualizaÃ§Ã£o de produtos do Mercado Livre, com foco em dispositivos Chromecast.

## ğŸ“Œ VisÃ£o Geral

O pipeline consiste em trÃªs etapas principais:
1. **ExtraÃ§Ã£o**: Coleta de dados da API de busca do Mercado Livre
2. **TransformaÃ§Ã£o e Carga**: Modelagem dos dados em estrutura relacional e carga em banco SQLite
3. **VisualizaÃ§Ã£o**: Dashboard interativo para anÃ¡lise dos dados

## ğŸ›  Tecnologias Utilizadas

- Python 3.8+
- SQLite
- Pandas
- Plotly
- Jupyter Notebook
- Requests

## ğŸ“‚ Estrutura do Projeto

<pre>
project-melichallenge/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_data.csv           # Dados brutos da API
â”‚   â””â”€â”€ mercado_livre.db       # Banco de dados SQLite
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extraction_meli_challenge.py          # CÃ³digo de extraÃ§Ã£o (Passo 1)
â”‚   â”œâ”€â”€ meli_challenge_ddl.sql                # DDL do modelo (Passo 2)
â”‚   â”œâ”€â”€ etl_meli_challenge.py                 # CÃ³digo ETL (Passo 2)
â”‚   â””â”€â”€ meli_challenge_dashboard.ipynb        # Jupyter notebook (Passo 3)
â”‚
â”œâ”€â”€ README.md                  # DocumentaÃ§Ã£o completa
â””â”€â”€ requirements.txt           # DependÃªncias do projeto
</pre>

## ğŸš€ Como Executar

### PrÃ©-requisitos

- Python 3.8 ou superior
- pip instalado

### InstalaÃ§Ã£o

1. Clone o repositÃ³rio:
```bash
git clone https://github.com/na-geciauskas/projeto-melichallenge.git
cd projeto-melichallenge
```

2. Instale as dependÃªncias:
```bash
python src/etl.py
```
## ğŸš€ ExecuÃ§Ã£o do Pipeline

### 1. ExtraÃ§Ã£o de dados:
```bash
python src/extraction.py

```
### 2. TransformaÃ§Ã£o e carga:
```bash
python src/etl.py
```

### 3. VisualizaÃ§Ã£o:
```bash
jupyter notebook src/dashboard.ipynb
```

## ğŸ” Detalhes de ImplementaÃ§Ã£o

### Passo 1: ExtraÃ§Ã£o
- Extrai 500 resultados da API de busca
- Termo de busca: "chromecast" (produto com volume suficiente para anÃ¡lise)
- Tratamento robusto de erros e rate limiting
- SaÃ­da: arquivo CSV com dados brutos

### Passo 2: Modelagem e Carga
- Modelo relacional com 4 tabelas normalizadas
- Script SQL para criaÃ§Ã£o da estrutura (DDL)
- Processo ETL para transformaÃ§Ã£o e carga dos dados
- Banco de dados SQLite como destino

### Passo 3: VisualizaÃ§Ã£o
- Dashboard interativo com:
  - Filtros por categoria e faixa de preÃ§o
  - Histograma de distribuiÃ§Ã£o de preÃ§os
  - GrÃ¡fico de dispersÃ£o preÃ§o vs vendidos
  - MÃ©tricas resumidas

## ğŸ“Š DecisÃµes de Projeto

**ExtraÃ§Ã£o**:
- PaginaÃ§Ã£o via parÃ¢metros `offset` e `limit`
- Tratamento de rate limiting (pausas automÃ¡ticas)
- 3 tentativas para cada requisiÃ§Ã£o

**Modelagem**:
- NormalizaÃ§Ã£o em 4 tabelas relacionadas
- Tipos de dados apropriados para anÃ¡lise
- Foco em atributos analÃ­ticos relevantes

**VisualizaÃ§Ã£o**:
- GrÃ¡ficos interativos com Plotly
- Filtros dinÃ¢micos com IPywidgets
- Layout simples e informativo

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ licenciado sob a licenÃ§a MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

## âœ‰ï¸ Contato

Para dÃºvidas ou sugestÃµes, usar um dos meios de contato:
- Email: na.geciauskas@gmail.com
- LinkedIn: [seu-perfil](https://linkedin.com/in/nara-geciauskas-ramos-castillo)

